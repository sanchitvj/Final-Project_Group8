# INITIAL SETTINGS

seed: 42
debug: False
save: False
resume_train: False

logger:
  project_name: 'feedback'
  key: 'd5d895d7e20cad280f1c9663b87cc40aa7eeb73b'
  save: False

dataset:
  data_path: 'train_df.csv'
  num_workers: 8
  tr_batch_size: 12  # base->12, large->2, bert->32, google->32, roberta->12
  vl_batch_size: 32  # base->32, large->8, bert->64, google->64, roberta->32
  max_len: 512  # 512  768  1024
  labels: ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']

model:
  backbone_name: 'roberta-large'  # 'microsoft/deberta-v3-base'  'roberta-large'
                   #  'google/electra-base-discriminator'  'bert-base-uncased'
  gradient_checkpointing: True
  pooling: 'mean'  # 'lstm'  'mean'  'concat'  'conv1d'
  saved_model_path: ''

  lstm_params:
    hidden_size: 1024
    dropout_rate: 0.1
    bidirectional: False

  concat_params:
    n_layers: 4

  conv1d_params:
    num_filters: 128
    kernel_size: 3

training:
  epochs: 10
  min_lr: 2e-6
  weight_decay: 1e-8
  eps: 1e-6
  betas: [0.9, 0.999]
  encoder_lr: 2e-5
  embeddings_lr: 7e-6
  decoder_lr: 3e-5
  max_grad_norm: 1000

