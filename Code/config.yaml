# INITIAL SETTINGS

seed: 12536
debug: False
save: False
resume_train: False

logger:
  project_name: 'feedback'
  key: 'd5d895d7e20cad280f1c9663b87cc40aa7eeb73b'
  save: False

dataset:
  data_path: 'train_df.csv'
  num_workers: 8
  tr_batch_size: 8
  vl_batch_size: 32
  max_len: 512
  labels: ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']

model:
  backbone_name: 'microsoft/deberta-v3-large'
  gradient_checkpointing: True
  pooling: 'lstm'  # 'lstm', 'mean'
  saved_model_path: ''

  lstm_params:
    hidden_size: 1024
    dropout_rate: 0.1
    bidirectional: True

training:
  epochs: 60
  min_lr: 2e-6
  weight_decay: 0.01
  eps: 1e-6
  betas: [0.9, 0.999]
  encoder_lr: 2e-5
  decoder_lr: 2e-5
  max_grad_norm: 1000

